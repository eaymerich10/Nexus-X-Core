from services.openai_service import ask_openai
# from services.local_model import ask_local  # <-- Lo activaremos en el futuro
import os

def get_response(history, user_input, mode="default", lang="es"):
    """
    Generates a response based on the provided conversation history and user input
    using the specified AI provider.

    Args:
        history (list): A list of previous conversation messages.
        user_input (str): The latest input from the user.
        mode (str, optional): The mode of operation for the AI provider. Defaults to "default".
        lang (str, optional): The language for the response. Defaults to "es".

    Returns:
        str: The response generated by the AI provider. If the AI provider is not implemented
             or unknown, an error message is returned.
    """

    ai_provider = os.getenv("AI_PROVIDER", "openai").lower()

    if ai_provider == "openai":
        return ask_openai(history, user_input, lang=lang, mode=mode)
    elif ai_provider == "local":
        # return ask_local(history, user_input, lang=lang, mode=mode)  # futuro
        return "[ERROR] Local model not yet implemented."
    else:
        return "[ERROR] Unknown AI provider."

from services.openai_service import ask_openai
# from services.local_model import ask_local  # <-- Lo activaremos en el futuro
import os

def get_response(
    history,
    user_input,
    mode="default",
    lang="es",
    max_tokens=80,
    extra_context=None,
    extra_interests=None
):
    """
    Generates a response based on the provided conversation history and user input
    using the specified AI provider.

    Args:
        history (list): A list of previous conversation messages.
        user_input (str): The latest input from the user.
        mode (str, optional): The mode of operation for the AI provider. Defaults to "default".
        lang (str, optional): The language for the response. Defaults to "es".
        max_tokens (int, optional): Maximum number of tokens for the response.
        extra_context (str, optional): Additional context to enrich the prompt.
        extra_interests (str, optional): Additional interest info to enrich the prompt.

    Returns:
        str: The response generated by the AI provider. If the AI provider is not implemented
             or unknown, an error message is returned.
    """

    ai_provider = os.getenv("AI_PROVIDER", "openai").lower()

    if ai_provider == "openai":
        return ask_openai(
            history,
            user_input,
            lang=lang,
            mode=mode,
            max_tokens=max_tokens,
            extra_context=extra_context,
            extra_interests=extra_interests
        )
    elif ai_provider == "local":
        # return ask_local(history, user_input, lang=lang, mode=mode)  # futuro
        return "[ERROR] Local model not yet implemented."
    else:
        return "[ERROR] Unknown AI provider."
